{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# The Drivers of Log Error in Single Unit Zestimates at Zillow\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "### Author\n",
    "- Samuel Davila\n",
    " - Data Scientist\n",
    " - Data Science Department\n",
    "\n",
    "### Data source\n",
    "- Single unit property data from Zillow table in Data Science Database\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "#### 1. Executive Summary\n",
    "\n",
    "#### 2. Data Acquisition\n",
    "\n",
    "#### 3. Preparation\n",
    "\n",
    "#### 4. Exploration\n",
    "\n",
    "#### 5. Modeling\n",
    "\n",
    "#### 6. Conclusion\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Executive Summary\n",
    "\n",
    "### Goals\n",
    "- Identify drivers of log error\n",
    "- Create a model that predicts log error\n",
    "    \n",
    "\n",
    "- Deliver the following:\n",
    "    - zillow_clustering_project.ipynb\n",
    "    - README.md\n",
    "    - acquire.py\n",
    "    - prep.py\n",
    "    - model.py\n",
    "    - A presentation that walks through each step of our project and summarizes this notebook.\n",
    "\n",
    "### Analysis\n",
    "\n",
    "Through data exploration and modeling, we found evidence that the following features are drivers of log error\n",
    "- bedroom_count\n",
    "- property_sq_ft\n",
    "- tax_dollar_value\n",
    "\n",
    "Clusters created from a combination of bedroom_count and property_sq_ft are viable predictors of log error\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "- Begin a project to improve the accuracy of our zillow estimate software using the insights and model generated from this project\n",
    "\n",
    "### Expectation\n",
    "\n",
    "- By improving the accuracy of our zestimates we will increase satisfaction among our current users and make our services more attractive to potential users. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire<a name=\"acquire\"></a>\n",
    "Acquire the data we need for our project from the zillow table in the data science database.\n",
    "\n",
    "Create __acquire.py__ file that contains the functions needed to replicate this process.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing modules needed for code in notebook to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "from acquire import get_zillow_data\n",
    "from prep import drop_missing_columns, missing_rows, drop_selected_columns, compare_column_values, tax_columns_calculator, split_data, handle_outliers, data_scaler, zillow_dummy, column_renamer, column_sort_rename, final_prep\n",
    "from model import baseline_function, model_1_function, model_2_function, model_3_function\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression, LassoLars\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using get_zillow_data function (acquire.py)  to import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parcelid</th>\n",
       "      <th>airconditioningtypeid</th>\n",
       "      <th>architecturalstyletypeid</th>\n",
       "      <th>basementsqft</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>buildingclasstypeid</th>\n",
       "      <th>buildingqualitytypeid</th>\n",
       "      <th>calculatedbathnbr</th>\n",
       "      <th>...</th>\n",
       "      <th>assessmentyear</th>\n",
       "      <th>landtaxvaluedollarcnt</th>\n",
       "      <th>taxamount</th>\n",
       "      <th>taxdelinquencyflag</th>\n",
       "      <th>taxdelinquencyyear</th>\n",
       "      <th>censustractandblock</th>\n",
       "      <th>id.1</th>\n",
       "      <th>parcelid.1</th>\n",
       "      <th>logerror</th>\n",
       "      <th>transactiondate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2288172</td>\n",
       "      <td>12177905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>36225.0</td>\n",
       "      <td>1777.51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.037300e+13</td>\n",
       "      <td>3</td>\n",
       "      <td>12177905</td>\n",
       "      <td>-0.10341</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970746</td>\n",
       "      <td>10887214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>45726.0</td>\n",
       "      <td>1533.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.037124e+13</td>\n",
       "      <td>4</td>\n",
       "      <td>10887214</td>\n",
       "      <td>0.00694</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  parcelid  airconditioningtypeid  architecturalstyletypeid  \\\n",
       "0  2288172  12177905                    NaN                       NaN   \n",
       "1  1970746  10887214                    1.0                       NaN   \n",
       "\n",
       "   basementsqft  bathroomcnt  bedroomcnt  buildingclasstypeid  \\\n",
       "0           NaN          3.0         4.0                  NaN   \n",
       "1           NaN          3.0         3.0                  NaN   \n",
       "\n",
       "   buildingqualitytypeid  calculatedbathnbr  ...  assessmentyear  \\\n",
       "0                    8.0                3.0  ...          2016.0   \n",
       "1                    8.0                3.0  ...          2016.0   \n",
       "\n",
       "   landtaxvaluedollarcnt  taxamount  taxdelinquencyflag  taxdelinquencyyear  \\\n",
       "0                36225.0    1777.51                 NaN                 NaN   \n",
       "1                45726.0    1533.89                 NaN                 NaN   \n",
       "\n",
       "   censustractandblock  id.1  parcelid.1  logerror  transactiondate  \n",
       "0         6.037300e+13     3    12177905  -0.10341       2017-01-01  \n",
       "1         6.037124e+13     4    10887214   0.00694       2017-01-01  \n",
       "\n",
       "[2 rows x 63 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create variable that will hold DF for easy access to data\n",
    "df = get_zillow_data()\n",
    "\n",
    "# previewing data\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47414, 63)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACQUIRE TAKEAWAYS\n",
    "- Acquiring data from zillow database on the data science database server using the __get_zillow_data__ function\n",
    "- Function needed to replicate this phase is located in the __acquire.py__ file\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare\n",
    "Prepare, tidy, and clean the data for exploration and analysis.\n",
    "\n",
    "Create __prep.py__ file that contains the functions needed to replicate this process.\n",
    "\n",
    "[Jump to RFE (presentation use only)](#rfe)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll use .info to see null value counts, data types, and row / columns count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47414 entries, 0 to 47413\n",
      "Data columns (total 63 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   id                            47414 non-null  int64  \n",
      " 1   parcelid                      47414 non-null  int64  \n",
      " 2   airconditioningtypeid         22070 non-null  float64\n",
      " 3   architecturalstyletypeid      0 non-null      float64\n",
      " 4   basementsqft                  0 non-null      float64\n",
      " 5   bathroomcnt                   47414 non-null  float64\n",
      " 6   bedroomcnt                    47414 non-null  float64\n",
      " 7   buildingclasstypeid           8 non-null      float64\n",
      " 8   buildingqualitytypeid         46923 non-null  float64\n",
      " 9   calculatedbathnbr             47368 non-null  float64\n",
      " 10  decktypeid                    0 non-null      float64\n",
      " 11  finishedfloor1squarefeet      0 non-null      float64\n",
      " 12  calculatedfinishedsquarefeet  47407 non-null  float64\n",
      " 13  finishedsquarefeet12          47366 non-null  float64\n",
      " 14  finishedsquarefeet13          0 non-null      float64\n",
      " 15  finishedsquarefeet15          36 non-null     float64\n",
      " 16  finishedsquarefeet50          0 non-null      float64\n",
      " 17  finishedsquarefeet6           5 non-null      float64\n",
      " 18  fips                          47414 non-null  float64\n",
      " 19  fireplacecnt                  0 non-null      float64\n",
      " 20  fullbathcnt                   47368 non-null  float64\n",
      " 21  garagecarcnt                  10 non-null     float64\n",
      " 22  garagetotalsqft               10 non-null     float64\n",
      " 23  hashottuborspa                0 non-null      float64\n",
      " 24  heatingorsystemtypeid         46685 non-null  float64\n",
      " 25  latitude                      47414 non-null  float64\n",
      " 26  longitude                     47414 non-null  float64\n",
      " 27  lotsizesquarefeet             46179 non-null  float64\n",
      " 28  poolcnt                       11603 non-null  float64\n",
      " 29  poolsizesum                   0 non-null      float64\n",
      " 30  pooltypeid10                  0 non-null      float64\n",
      " 31  pooltypeid2                   0 non-null      float64\n",
      " 32  pooltypeid7                   11582 non-null  float64\n",
      " 33  propertycountylandusecode     47414 non-null  object \n",
      " 34  propertylandusetypeid         47414 non-null  float64\n",
      " 35  propertyzoningdesc            47184 non-null  object \n",
      " 36  rawcensustractandblock        47414 non-null  float64\n",
      " 37  regionidcity                  46372 non-null  float64\n",
      " 38  regionidcounty                47414 non-null  float64\n",
      " 39  regionidneighborhood          24146 non-null  float64\n",
      " 40  regionidzip                   47394 non-null  float64\n",
      " 41  roomcnt                       47414 non-null  float64\n",
      " 42  storytypeid                   0 non-null      float64\n",
      " 43  threequarterbathnbr           0 non-null      float64\n",
      " 44  typeconstructiontypeid        0 non-null      float64\n",
      " 45  unitcnt                       47414 non-null  float64\n",
      " 46  yardbuildingsqft17            0 non-null      float64\n",
      " 47  yardbuildingsqft26            0 non-null      float64\n",
      " 48  yearbuilt                     47400 non-null  float64\n",
      " 49  numberofstories               20 non-null     float64\n",
      " 50  fireplaceflag                 0 non-null      float64\n",
      " 51  structuretaxvaluedollarcnt    47350 non-null  float64\n",
      " 52  taxvaluedollarcnt             47414 non-null  float64\n",
      " 53  assessmentyear                47414 non-null  float64\n",
      " 54  landtaxvaluedollarcnt         47414 non-null  float64\n",
      " 55  taxamount                     47410 non-null  float64\n",
      " 56  taxdelinquencyflag            2210 non-null   object \n",
      " 57  taxdelinquencyyear            2210 non-null   float64\n",
      " 58  censustractandblock           47298 non-null  float64\n",
      " 59  id.1                          47414 non-null  int64  \n",
      " 60  parcelid.1                    47414 non-null  int64  \n",
      " 61  logerror                      47414 non-null  float64\n",
      " 62  transactiondate               47414 non-null  object \n",
      "dtypes: float64(55), int64(4), object(4)\n",
      "memory usage: 22.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# using info function to examine data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many columns have a large amount of null values\n",
    "    - We'll be using our upcoming function, __drop_missing_columns__ (prep.py) to drop columns with 40% or more of their values missing\n",
    "    - We could impute them, but with so many values missing, I'm wary that the values we choose to impute them with may not be a good representation of the population\n",
    "\n",
    "\n",
    "- Data types of several columns need to be converted \n",
    "    - We'll handle this in a later stage since the columns in question may be removed in an alternate step \n",
    "        - For example, we may remove a column with a bad data type due to it having too many nulls\n",
    "\n",
    "\n",
    "- Several columns, such as rawcensustractandblock, are categorical variables that may have a very large amount of unique values\n",
    "    - Encoding every value for these types of columns may be computationally expensive and add a large amount of columns to our dataset\n",
    "        \n",
    "        \n",
    "- The transaction date is only known when the property is sold, we're trying to created a model that Zillow can use on properties before they have been sold\n",
    "    - transactiondate will be dropped as it will not be useful in a real-world setting as a result of this\n",
    "\n",
    "\n",
    "- Dropping tax_amount as it was specified that this column was not allowed to be used in the given project specs\n",
    "\n",
    "\n",
    "\n",
    "- Once we've identified which columns to move into explore with, we'll need to rename them if they are hard to read, such as landtaxvaluedollarcnt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are several columns with large amounts of values missing. We could impute them, but with so many rows missing, using the average or median within them may not be indicative of Using drop_missing_columns function (prep.py) to remove columns that are missing 40% or more of their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parcelid</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>buildingqualitytypeid</th>\n",
       "      <th>calculatedbathnbr</th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>finishedsquarefeet12</th>\n",
       "      <th>fips</th>\n",
       "      <th>fullbathcnt</th>\n",
       "      <th>...</th>\n",
       "      <th>structuretaxvaluedollarcnt</th>\n",
       "      <th>taxvaluedollarcnt</th>\n",
       "      <th>assessmentyear</th>\n",
       "      <th>landtaxvaluedollarcnt</th>\n",
       "      <th>taxamount</th>\n",
       "      <th>censustractandblock</th>\n",
       "      <th>id.1</th>\n",
       "      <th>parcelid.1</th>\n",
       "      <th>logerror</th>\n",
       "      <th>transactiondate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2288172</td>\n",
       "      <td>12177905</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>108918.0</td>\n",
       "      <td>145143.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>36225.0</td>\n",
       "      <td>1777.51</td>\n",
       "      <td>6.037300e+13</td>\n",
       "      <td>3</td>\n",
       "      <td>12177905</td>\n",
       "      <td>-0.103410</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970746</td>\n",
       "      <td>10887214</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>73681.0</td>\n",
       "      <td>119407.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>45726.0</td>\n",
       "      <td>1533.89</td>\n",
       "      <td>6.037124e+13</td>\n",
       "      <td>4</td>\n",
       "      <td>10887214</td>\n",
       "      <td>0.006940</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>781532</td>\n",
       "      <td>12095076</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2962.0</td>\n",
       "      <td>2962.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>276684.0</td>\n",
       "      <td>773303.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>496619.0</td>\n",
       "      <td>9516.26</td>\n",
       "      <td>6.037461e+13</td>\n",
       "      <td>6</td>\n",
       "      <td>12095076</td>\n",
       "      <td>-0.001011</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>870991</td>\n",
       "      <td>12069064</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>738.0</td>\n",
       "      <td>738.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18890.0</td>\n",
       "      <td>218552.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>199662.0</td>\n",
       "      <td>2366.08</td>\n",
       "      <td>6.037302e+13</td>\n",
       "      <td>7</td>\n",
       "      <td>12069064</td>\n",
       "      <td>0.101723</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1246926</td>\n",
       "      <td>12790562</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3039.0</td>\n",
       "      <td>3039.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>177527.0</td>\n",
       "      <td>220583.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>43056.0</td>\n",
       "      <td>3104.19</td>\n",
       "      <td>6.037500e+13</td>\n",
       "      <td>8</td>\n",
       "      <td>12790562</td>\n",
       "      <td>-0.040966</td>\n",
       "      <td>2017-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  parcelid  bathroomcnt  bedroomcnt  buildingqualitytypeid  \\\n",
       "0  2288172  12177905          3.0         4.0                    8.0   \n",
       "1  1970746  10887214          3.0         3.0                    8.0   \n",
       "2   781532  12095076          3.0         4.0                    9.0   \n",
       "3   870991  12069064          1.0         2.0                    5.0   \n",
       "4  1246926  12790562          3.0         4.0                    9.0   \n",
       "\n",
       "   calculatedbathnbr  calculatedfinishedsquarefeet  finishedsquarefeet12  \\\n",
       "0                3.0                        2376.0                2376.0   \n",
       "1                3.0                        1312.0                1312.0   \n",
       "2                3.0                        2962.0                2962.0   \n",
       "3                1.0                         738.0                 738.0   \n",
       "4                3.0                        3039.0                3039.0   \n",
       "\n",
       "     fips  fullbathcnt  ...  structuretaxvaluedollarcnt  taxvaluedollarcnt  \\\n",
       "0  6037.0          3.0  ...                    108918.0           145143.0   \n",
       "1  6037.0          3.0  ...                     73681.0           119407.0   \n",
       "2  6037.0          3.0  ...                    276684.0           773303.0   \n",
       "3  6037.0          1.0  ...                     18890.0           218552.0   \n",
       "4  6037.0          3.0  ...                    177527.0           220583.0   \n",
       "\n",
       "   assessmentyear  landtaxvaluedollarcnt taxamount  censustractandblock id.1  \\\n",
       "0          2016.0                36225.0   1777.51         6.037300e+13    3   \n",
       "1          2016.0                45726.0   1533.89         6.037124e+13    4   \n",
       "2          2016.0               496619.0   9516.26         6.037461e+13    6   \n",
       "3          2016.0               199662.0   2366.08         6.037302e+13    7   \n",
       "4          2016.0                43056.0   3104.19         6.037500e+13    8   \n",
       "\n",
       "   parcelid.1  logerror  transactiondate  \n",
       "0    12177905 -0.103410       2017-01-01  \n",
       "1    10887214  0.006940       2017-01-01  \n",
       "2    12095076 -0.001011       2017-01-01  \n",
       "3    12069064  0.101723       2017-01-01  \n",
       "4    12790562 -0.040966       2017-01-02  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function removes any columns that are missing 40% or more of their values\n",
    "df = drop_missing_columns(df)\n",
    "\n",
    "# previewing data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While examining the data in SQL, we noticed that several columns appeared to have identical values. \n",
    "\n",
    "### Using compare_column_values (prep.py) to find how many unique values exist between the columns in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different values between finishedsquarefeet12 and calculatedfinishedsquarefeet: 48\n",
      "Number of different values between calculatedbathnbr and bathroomcnt: 46\n",
      "Number of different values between fullbathcnt and bathroomcnt: 46\n"
     ]
    }
   ],
   "source": [
    "# function prints out sum of unique values between various columns\n",
    "compare_column_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The sum of non-duplicate values between all of these columns is 140 (48 + 46 + 46). \n",
    "    - This is roughly .003 of all rows in the DF\n",
    "        - We can safely drop all the following majority-duplicate columns and only lose a extremely small proportion of rows.\n",
    "            - finishedsquarefeet12\n",
    "            - calculatedbathnbr\n",
    "            - fullbathcnt\n",
    "        - We could drop any of their alternativee columns instead but it would cost us time to find a non-arbitrary reason to do so and given how few unique values we're losing, the loss is relatively inconsequential.\n",
    "        - These columns will be dropped in an upcoming function, __drop_selected_columns__ (prep.py), along with any other columns that are found to be in need of removal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While examining the data in SQL, we noticed that taxvaluedollarcnt appeared to be the sum of landtaxvaluedollarcnt and structuretaxvaluedollarcnt. \n",
    "\n",
    "### To test this we're going to use tax_columns_calculator (prep.py) to compare the sums of landtaxvaluedollarcnt and structuretaxvaluedollarcnt  to the values in taxvaluedollarcnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9986501877082719\n"
     ]
    }
   ],
   "source": [
    "# function sums landtaxvaluedollarcnt and structuretaxvaluedollarcnt then prints the percent of rows where the sum matched taxvaluedollarcnt\n",
    "tax_columns_calculator(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 99% of the our summed values matched the original \n",
    "    - In a vast majority of rows, taxvaluedollarcnt is the sum of landtaxvaluedollarcnt and structuretaxvaluedollarcnt\n",
    "        - This being the case, we're only going to keep taxvaluedollarcnt and remove the other two columns since their values are already accounted for in this column.\n",
    "        - If need be, we can add them back later and see if we get better results by having them seperated.\n",
    "        - The columns will be dropped at a later step using the __drop_selected_columns__ (prep.py) function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're using .nunique to see how many unique values each column has. This is useful for identifying categorical columns with large amounts of unique values and columns with only a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                              47293\n",
       "parcelid                        47293\n",
       "bathroomcnt                        13\n",
       "bedroomcnt                         12\n",
       "buildingqualitytypeid              12\n",
       "calculatedbathnbr                  12\n",
       "calculatedfinishedsquarefeet     4302\n",
       "finishedsquarefeet12             4300\n",
       "fips                                3\n",
       "fullbathcnt                        12\n",
       "heatingorsystemtypeid               3\n",
       "latitude                        38668\n",
       "longitude                       37041\n",
       "lotsizesquarefeet               16506\n",
       "propertycountylandusecode          40\n",
       "propertylandusetypeid              10\n",
       "propertyzoningdesc               1854\n",
       "rawcensustractandblock          25212\n",
       "regionidcity                      135\n",
       "regionidcounty                      3\n",
       "regionidzip                       290\n",
       "roomcnt                             5\n",
       "unitcnt                             1\n",
       "yearbuilt                         131\n",
       "structuretaxvaluedollarcnt      28782\n",
       "taxvaluedollarcnt               32671\n",
       "assessmentyear                      1\n",
       "landtaxvaluedollarcnt           28407\n",
       "taxamount                       46062\n",
       "censustractandblock             25234\n",
       "id.1                            47414\n",
       "parcelid.1                      47293\n",
       "logerror                        47020\n",
       "transactiondate                   253\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nunique() displays each column and the amount of unique values that it holds\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following are categorical columns will be removed because they contain more than 10 unique values. Encoding them would be computationaly expensive and add a large amount of columns to our dataframe.\n",
    "\n",
    "    - id\n",
    "    - parcelid\n",
    "    - propertycountylandusecode\n",
    "    - propertyzoningdesc\n",
    "    - rawcensustractandblock\n",
    "    - regionidcity\n",
    "    - regionidzip\n",
    "    - censustractandblock\n",
    "    - id.1\n",
    "    - parcelid.1 \n",
    "    \n",
    "- There are ways to avoid the consequences of encoding categorical columns with lots of features, but in the interest of time we will avoid these routes for now.\n",
    "\n",
    "\n",
    "- Will also be dropping columns with only 1 unique value that don't provide any useful information\n",
    "\n",
    "\n",
    "- All of these columns will be removed using the __drop_selected_columns__ (prep.py) function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using __drop_selected_columns__ to drop columns listed in the sections above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>buildingqualitytypeid</th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>heatingorsystemtypeid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>lotsizesquarefeet</th>\n",
       "      <th>propertylandusetypeid</th>\n",
       "      <th>yearbuilt</th>\n",
       "      <th>taxvaluedollarcnt</th>\n",
       "      <th>taxamount</th>\n",
       "      <th>logerror</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34245180.0</td>\n",
       "      <td>-118240722.0</td>\n",
       "      <td>13038.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>145143.0</td>\n",
       "      <td>1777.51</td>\n",
       "      <td>-0.10341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34185120.0</td>\n",
       "      <td>-118414640.0</td>\n",
       "      <td>278581.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>119407.0</td>\n",
       "      <td>1533.89</td>\n",
       "      <td>0.00694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bathroomcnt  bedroomcnt  buildingqualitytypeid  \\\n",
       "0          3.0         4.0                    8.0   \n",
       "1          3.0         3.0                    8.0   \n",
       "\n",
       "   calculatedfinishedsquarefeet  heatingorsystemtypeid    latitude  \\\n",
       "0                        2376.0                    2.0  34245180.0   \n",
       "1                        1312.0                    2.0  34185120.0   \n",
       "\n",
       "     longitude  lotsizesquarefeet  propertylandusetypeid  yearbuilt  \\\n",
       "0 -118240722.0            13038.0                  261.0     1970.0   \n",
       "1 -118414640.0           278581.0                  266.0     1964.0   \n",
       "\n",
       "   taxvaluedollarcnt  taxamount  logerror  \n",
       "0           145143.0    1777.51  -0.10341  \n",
       "1           119407.0    1533.89   0.00694  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function drops various columns\n",
    "drop_selected_columns(df)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some of our columns are probably still missing data so we'll use our missing_rows (prep.py) function again to find what overall percent of rows are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At most, 5.0% of rows contain missing values.\n"
     ]
    }
   ],
   "source": [
    "# passing dataframe to missing_rows function\n",
    "missing_rows_df = missing_rows(df)\n",
    "\n",
    "# summing values in the percent of rows missing column\n",
    "total_percent_rows_missing = round((missing_rows_df.pct_rows_missing).sum(),0)\n",
    "\n",
    "print(f'At most, {total_percent_rows_missing}% of rows contain missing values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given how small this percentage is, it should be relatively inconsequential to drop rows with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping remaining rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows with missing values\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating data types as needed for our operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating dictionary to specify data type changes\n",
    "convert_dict = {'bathroomcnt': int, \n",
    "                'bedroomcnt': int,\n",
    "                'buildingqualitytypeid': int,\n",
    "                'heatingorsystemtypeid': int,\n",
    "                'latitude': int,\n",
    "                'longitude': int,\n",
    "                'lotsizesquarefeet': int,\n",
    "                'propertylandusetypeid': int,\n",
    "                'yearbuilt': int,\n",
    "               } \n",
    "  \n",
    "# passing dictionary to perform dtype updates\n",
    "df = df.astype(convert_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Updating float columns that have no non-zero values after decimal to integer\n",
    "- After this change, all columns with have an approriate type for the operations we intend to perform on them\n",
    "    - May need to update some later if their typing is not a good fit for new operations we introduce "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using zillow_dummy function (prep.py) to create dummy variables for heatingorsystemtypeid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>buildingqualitytypeid</th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>heatingorsystemtypeid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>lotsizesquarefeet</th>\n",
       "      <th>propertylandusetypeid</th>\n",
       "      <th>yearbuilt</th>\n",
       "      <th>...</th>\n",
       "      <th>heatingorsystemtypeid_20</th>\n",
       "      <th>propertylandusetypeid_31</th>\n",
       "      <th>propertylandusetypeid_246</th>\n",
       "      <th>propertylandusetypeid_247</th>\n",
       "      <th>propertylandusetypeid_260</th>\n",
       "      <th>propertylandusetypeid_261</th>\n",
       "      <th>propertylandusetypeid_264</th>\n",
       "      <th>propertylandusetypeid_266</th>\n",
       "      <th>propertylandusetypeid_267</th>\n",
       "      <th>propertylandusetypeid_269</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>2</td>\n",
       "      <td>34245180</td>\n",
       "      <td>-118240722</td>\n",
       "      <td>13038</td>\n",
       "      <td>261</td>\n",
       "      <td>1970</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>2</td>\n",
       "      <td>34185120</td>\n",
       "      <td>-118414640</td>\n",
       "      <td>278581</td>\n",
       "      <td>266</td>\n",
       "      <td>1964</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bathroomcnt  bedroomcnt  buildingqualitytypeid  \\\n",
       "0            3           4                      8   \n",
       "1            3           3                      8   \n",
       "\n",
       "   calculatedfinishedsquarefeet  heatingorsystemtypeid  latitude  longitude  \\\n",
       "0                        2376.0                      2  34245180 -118240722   \n",
       "1                        1312.0                      2  34185120 -118414640   \n",
       "\n",
       "   lotsizesquarefeet  propertylandusetypeid  yearbuilt  ...  \\\n",
       "0              13038                    261       1970  ...   \n",
       "1             278581                    266       1964  ...   \n",
       "\n",
       "   heatingorsystemtypeid_20  propertylandusetypeid_31  \\\n",
       "0                         0                         0   \n",
       "1                         0                         0   \n",
       "\n",
       "   propertylandusetypeid_246  propertylandusetypeid_247  \\\n",
       "0                          0                          0   \n",
       "1                          0                          0   \n",
       "\n",
       "   propertylandusetypeid_260  propertylandusetypeid_261  \\\n",
       "0                          0                          1   \n",
       "1                          0                          0   \n",
       "\n",
       "   propertylandusetypeid_264  propertylandusetypeid_266  \\\n",
       "0                          0                          0   \n",
       "1                          0                          1   \n",
       "\n",
       "   propertylandusetypeid_267  propertylandusetypeid_269  \n",
       "0                          0                          0  \n",
       "1                          0                          0  \n",
       "\n",
       "[2 rows x 36 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving resulting df to variable\n",
    "df = zillow_dummy(df)\n",
    "\n",
    "# previewing dummy data\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dummy variables added succesfully, now we can split the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort and rename columns\n",
    "Tidying columns via sorting (ie. boolean columns and their sources are next to each other, target variable is last column in DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = column_sort_rename(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using split_data function (prep.py) to split data into train, validate and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25255, 36)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function splits data into train, validate and test samples\n",
    "train, validate, test = split_data(df)\n",
    "\n",
    "# using shape to see count of rows and columns\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use .shape to confirm our number of rows reflects a split and it has"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTLIERS\n",
    "Edit: I've explored and modeled the data with and without extreme outliers (extreme outliers are defined below) and found that removing outliers actually made my models perform a lot worse while not benefitting the project. I'm going to leave them in for this iteration of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now use our handle_outliers function (prep.py) to add remove any rows that contain an outlier from their respective non-target variable column. We're only looking for the most extreme outliers so the \"cutoff\" value we're using is 6.\n",
    "\n",
    "- Lower outlier = q1 - (cutoff value * IQR)\n",
    "- Upper outlier = q3 + (cutoff value * IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle_outliers(train, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Outlier columns added successfully\n",
    "- Although present, we're disregarding all heating systems outliers because they are categorical\n",
    "- We're also disregarding logerror because we are not removing outliers from our target variable\n",
    "    - We want our project to reflect the fact that we can't remove outliers in many real-world scenarios\n",
    "- Set k to 6 which means that only the most extreme outliers will be identified\n",
    "    - Normally k is set to 1.5 but when we tried this value earlier, we identified so many outliers than removing them all would cost us too much data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We don't need our outlier columns going forward so we'll drop them to simplify our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating list of outlier column names\n",
    "#outlier_cols = [col for col in train if col.endswith('_outliers')]\n",
    "\n",
    "# dropping each column from list of outlier columns\n",
    "#train = train.drop(columns = outlier_cols)\n",
    "\n",
    "# previewing data\n",
    "#train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Columns succesfully dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCALING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using data_scaler (prep.py) to scale the data within our non-categorical and non-target variable columns.\n",
    "### We'll be keeping a set of unscaled dataframes for things such as visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathroom_count</th>\n",
       "      <th>bedroom_count</th>\n",
       "      <th>property_sqft</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>lotsize_sqft</th>\n",
       "      <th>year_built</th>\n",
       "      <th>tax_dollar_value</th>\n",
       "      <th>tax_amount</th>\n",
       "      <th>building_quality_type_id</th>\n",
       "      <th>...</th>\n",
       "      <th>propertylandusetypeid_31</th>\n",
       "      <th>propertylandusetypeid_246</th>\n",
       "      <th>propertylandusetypeid_247</th>\n",
       "      <th>propertylandusetypeid_260</th>\n",
       "      <th>propertylandusetypeid_261</th>\n",
       "      <th>propertylandusetypeid_264</th>\n",
       "      <th>propertylandusetypeid_266</th>\n",
       "      <th>propertylandusetypeid_267</th>\n",
       "      <th>propertylandusetypeid_269</th>\n",
       "      <th>logerror</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44417</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.044127</td>\n",
       "      <td>0.461915</td>\n",
       "      <td>0.407417</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.321168</td>\n",
       "      <td>0.026811</td>\n",
       "      <td>0.028135</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9318</th>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.063271</td>\n",
       "      <td>0.723460</td>\n",
       "      <td>0.357351</td>\n",
       "      <td>0.020376</td>\n",
       "      <td>0.832117</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>0.013703</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.015744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bathroom_count  bedroom_count  property_sqft  latitude  longitude  \\\n",
       "44417        0.090909       0.222222       0.044127  0.461915   0.407417   \n",
       "9318         0.181818       0.222222       0.063271  0.723460   0.357351   \n",
       "\n",
       "       lotsize_sqft  year_built  tax_dollar_value  tax_amount  \\\n",
       "44417      0.000477    0.321168          0.026811    0.028135   \n",
       "9318       0.020376    0.832117          0.010971    0.013703   \n",
       "\n",
       "       building_quality_type_id  ...  propertylandusetypeid_31  \\\n",
       "44417                         4  ...                         0   \n",
       "9318                          8  ...                         0   \n",
       "\n",
       "       propertylandusetypeid_246  propertylandusetypeid_247  \\\n",
       "44417                          0                          0   \n",
       "9318                           0                          0   \n",
       "\n",
       "       propertylandusetypeid_260  propertylandusetypeid_261  \\\n",
       "44417                          0                          1   \n",
       "9318                           0                          0   \n",
       "\n",
       "       propertylandusetypeid_264  propertylandusetypeid_266  \\\n",
       "44417                          0                          0   \n",
       "9318                           0                          1   \n",
       "\n",
       "       propertylandusetypeid_267  propertylandusetypeid_269  logerror  \n",
       "44417                          0                          0  0.020245  \n",
       "9318                           0                          0 -0.015744  \n",
       "\n",
       "[2 rows x 36 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaling appropriate numerical columns\n",
    "train_scaled, validate_scaled, test_scaled = data_scaler(train, validate, test)\n",
    "\n",
    "# previewing dataframe\n",
    "train_scaled.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data scaled successfully\n",
    "- We're not scaling any of our categorical columns \n",
    "    - They are all binary values between 0 and 1 and thus don't need scaling\n",
    "- We're not scaling logerror because its our target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARE TAKEAWAYS\n",
    "- Dropped all columns that met any of the following criterion\n",
    "    - 40% or more of their values missing\n",
    "    - Duplicate or near-duplicate of other column(s)\n",
    "    - Categorical with 10 or more unique values\n",
    "    - Only contain 1 unique value\n",
    "    - landtaxvaluedollarcnt and structuretaxvaluedollarcnt\n",
    "        - Summed under taxvaluedollarcnt column\n",
    "    - Was not found to be a top feature after RFE was run\n",
    "\n",
    "\n",
    "- Dropped all rows with missing values\n",
    "\n",
    "\n",
    "- Removed all extreme upper outliers from dataset \n",
    "\n",
    "\n",
    "- Scaled all numeric non-categorical, non-target variables\n",
    "\n",
    "\n",
    "- Data types are valid\n",
    "\n",
    "\n",
    "- Split data into train, validate and test sets\n",
    "\n",
    "\n",
    "- Renamed columns for readability\n",
    "\n",
    "\n",
    "- Columns we are moving into explore with are\n",
    "    - bedroom_count\t\n",
    "    - property_sq_ft\t\n",
    "    - tax_dollar_value\t\n",
    "    - log_error\n",
    "\n",
    "\n",
    "- All custom functions in this phase can be found in the __prepare.py__ and __prep.py__ files\n",
    "    - When each function is appears in the notebook, the file it's located in is displayed next to it's name in ()\n",
    "    - To retrieve the data fully prepared without any of the previous functions, you can alternatively run the __final_prep__ function (no arguments needed) from the __prep.py__ file\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore<a name=\"explore\"></a>\n",
    "We will now explore our prepped data to identify drivers of logerror and relationships between variables.\n",
    "\n",
    "We will be using clustering to aid us in the latter half of our exploration.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Clustered Exploration\n",
    "\n",
    "Let's begin our exploration by creating using visualizations and hypothesis tests to gauge the relationships between our features and our target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: property_sq_ft\n",
    "Let's explore how the square feet of a property relates to log error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization - property_sq_ft<a name=\"nce\"></a>\n",
    "#### We'll begin by plotting the relationship between property_sq_ft and log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting figure and text size for plot\n",
    "sns.set(rc = {'figure.figsize':(12,6)})\n",
    "sns.set(font_scale = 2)\n",
    "\n",
    "# creating scatter plot and setting title\n",
    "sns.scatterplot(x = 'property_sq_ft', y = 'log_error', data = train).set_title(\"Lower Property SqFt. Relates to Worse Log Error Values\")\n",
    "\n",
    "# setting x and y labels\n",
    "plt.xlabel(\"Property Size (SqFt.)\")\n",
    "plt.ylabel(\"Log Error of Zestimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The farther away from 0 a zestimate's log error is, the farther off from reality the zestimate was\n",
    "\n",
    "- Generally, the smaller properties have log error values farther from 0 and thus different on average than larger properties\n",
    "    - We'll perform a hypothesis test based on this observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Test - property_sq_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll now perform a two tailed, two sample t-test to see if smaller properties have different log error values than larger properties.\n",
    "\n",
    "- H0 = Log error of properties with below avg. property_sq_ft __=__ Log error of properties with avg. or above avg. property_sq_ft\n",
    "- Ha = Log error of properties with below avg. property_sq_ft __!=__ Log error of properties with avg. or above avg. property_sq_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding avg property_sq_ft\n",
    "avg_property_sq_ft = train.property_sq_ft.mean()\n",
    "\n",
    "# x1 = df of properties with property_sq_ft less than the avg.\n",
    "# x2 = df of properties with property_sq_ft greater than or equal to the avg.\n",
    "x1 = train[train.property_sq_ft < avg_property_sq_ft].log_error\n",
    "x2 = train[train.property_sq_ft >= avg_property_sq_ft].log_error\n",
    "\n",
    "# performing 2 tailed, 2 sample t-test\n",
    "t, p = stats.ttest_ind(x1, x2)\n",
    "\n",
    "# printing results\n",
    "print(f'alpha = .05\\n')\n",
    "print (f'p = {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- p is below our alpha (.05) so we reject our null hypothesis and conclude there is evidence that properties with less than avg. square feet have different log error values than properties with avg. or above avg. square feet\n",
    "\n",
    "- In light of this, we'll keep this feature as a candidate for modeling\n",
    "\n",
    "[Jump to Clustered Exploration (For Presentation Use Only)](#ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: tax_dollar_value\n",
    "Let's now explore how tax dollar value relates to log error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization - tax_dollar_value\n",
    "#### We will plot the relationship between tax_dollar_value and log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting figure and text size\n",
    "sns.set(rc = {'figure.figsize':(12,6)})\n",
    "sns.set(font_scale = 2)\n",
    "\n",
    "# plotting data, setting title and scaling x axis values to simplify them\n",
    "sns.scatterplot(x = train['tax_dollar_value'] / 1000, y = 'log_error', data = train).set_title(\"Lower Tax Dollar Value Relates to Worse Log Error Values\")\n",
    "plt.xlabel(\"Tax Value in Dollars (Thousands)\")\n",
    "plt.ylabel(\"Log Error of Zestimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This visual looks very similar to our previous one and has similar takeaways\n",
    "\n",
    "- The farther away from 0 a zestimate's log error is, the farther off from reality the zestimate was\n",
    "\n",
    "- The lower value properties appear to have more log error values that are more distant from 0 than the higher value properties\n",
    "       \n",
    "- We'll perform a hypothesis test to see if lower value properties tend to have different log error values than high value properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Test - tax_dollar_value\n",
    "\n",
    "#### Let's perform a two tailed, two sample t-test to see if properties with lower tax dollar values have different log error values than higher tax dollar value properties.\n",
    "\n",
    "- H0 = Log error of properties with below avg. tax_dollar_value __=__ Log error of properties with avg. or above avg. tax_dollar_value\n",
    "- Ha = Log error of properties with below avg. tax_dollar_value __!=__ Log error of properties with avg. or above avg. tax_dollar_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding avg tax dollar value\n",
    "avg_tax_dollar_value = train.tax_dollar_value.mean()\n",
    "\n",
    "# x1 = df of properties with tax dollar value less than the avg.\n",
    "# x2 = df of properties with tax dollar value greater than or equal to the avg.\n",
    "x1 = train[train.tax_dollar_value < avg_tax_dollar_value].log_error\n",
    "x2 = train[train.tax_dollar_value >= avg_tax_dollar_value].log_error\n",
    "\n",
    "# performing 2 tailed, 2 sample t-test\n",
    "t, p = stats.ttest_ind(x1, x2)\n",
    "\n",
    "# printing results\n",
    "print(f'alpha = .05\\n')\n",
    "print (f'p = {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- p is below our alpha (.05) so we reject our null hypothesis and conclude there is evidence that properties with lower sqft. have different log error values than high value properties\n",
    "       \n",
    "- Based on these observations, tax_dollar_value will remain a prospective feature for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: bedroom_count\n",
    "Let's now explore how the number of bedrooms in a property relates to log error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization - bedroom_count\n",
    "#### We will plot the relationship between bedroom_count and log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting figure size\n",
    "sns.set(rc = {'figure.figsize':(12,6)})\n",
    "\n",
    "# setting font size\n",
    "sns.set(font_scale = 2)\n",
    "\n",
    "# plotting scatterplot and setting title\n",
    "sns.scatterplot(x = 'bedroom_count', y = 'log_error', data = train).set_title(\"Lower Bedroom Count Relates to Worse Log Error Values\")\n",
    "\n",
    "# setting x and y label\n",
    "plt.xlabel(\"Number of Bedrooms in Property\")\n",
    "plt.ylabel(\"Log Error of Zestimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similar to our other features, we find that as the number of bedrooms decreases, our log error values pull farther away from 0\n",
    "    - In other words, it appears that the less bedrooms a property has, the worse our zestimate tends to be\n",
    "        - Interestingly, this does not appear to be true for properties with no bedrooms\n",
    "            - If a property has no bedrooms, it may be a special kind of property that we are able to evaluate more effectively\n",
    "            \n",
    "            \n",
    "- To support our observation we'll perform a hypothesis test to see if properties with lower amounts of bedrooms have different log error values than properties with higher amounts of bedrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Test - bedroom_count\n",
    "\n",
    "#### Let's perform a two tailed, two sample t-test to see if properties with less bedrooms have different log error values than properties with more bedrooms.\n",
    "\n",
    "- H0 = Log error of properties with below avg. bedroom_count __=__ Log error of properties with avg. or above avg. bedroom_count\n",
    "- Ha = Log error of properties with below avg. bedroom_count __!=__ Log error of properties with avg. or above avg. bedroom_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding avg bedroom_count\n",
    "avg_bedroom_count = train.bedroom_count.mean()\n",
    "\n",
    "# x1 = df of properties with bedroom_count less than the avg.\n",
    "# x2 = df of properties with bedroom_countgreater than or equal to the avg.\n",
    "x1 = train[train.bedroom_count < avg_bedroom_count].log_error\n",
    "x2 = train[train.bedroom_count >= avg_bedroom_count].log_error\n",
    "\n",
    "# performing 2 tailed, 2 sample t-test\n",
    "t, p = stats.ttest_ind(x1, x2)\n",
    "\n",
    "# printing results\n",
    "print(f'alpha = .05\\n')\n",
    "print (f'p = {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- p is below our alpha (.05) so we reject our null hypothesis and conclude there is evidence that properties with lower than avg. bedroom counts have different log errors than properties with the avg. or more than the avg. number of bedrooms\n",
    "\n",
    "- Based on our findings, bedroom_count should be considered a lucrative feature for predicting log error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Exploration\n",
    "\n",
    "Let's continue exploring by creating clusters that we can create hypothesis tests and visualizations with in order to gain further insight into the drivers of log_error.\n",
    "\n",
    "It's important to cluster on scaled data when possible so we'll be using our scaled dataset here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Feature Set 1 - property_sq_ft | tax_dollar_value\n",
    "\n",
    "Let's cluster using these two features and check if there is any difference between their log error values.\n",
    "\n",
    "To begin, well use subplots to see what our clusters look like with different cluster amounts. This gives us a good idea of how many clusters we should choose for a given pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating scaled and unscaled DFs with the two features we want to cluster\n",
    "# scaled for clustering, unscaled for plotting\n",
    "X_c1 = train_scaled[['property_sq_ft', 'tax_dollar_value']]\n",
    "X_c1_unscaled = train[['property_sq_ft', 'tax_dollar_value']]\n",
    "\n",
    "# setting up subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12,12), sharex=True, sharey=True)\n",
    "\n",
    "# creating clusters, number of clusters increasing from 2 - 5\n",
    "for ax, k in zip(axs.ravel(), range(2, 6)):\n",
    "    clusters = KMeans(k).fit(X_c1).predict(X_c1)\n",
    "    X_c1_unscaled['clusters'] = clusters\n",
    "    plt.suptitle('Varying K Values for Cluster: tax_dollar_value | property_sq_ft',fontsize=25)\n",
    "    \n",
    "    # plotting \n",
    "    sns.set(font_scale = 2)\n",
    "    ax.scatter(X_c1_unscaled.property_sq_ft, X_c1_unscaled.tax_dollar_value / 1000, c=clusters, cmap = 'tab10')\n",
    "    ax.set(title='Cluster Count = {}'.format(k), xlabel='Property Size (SqFt.)', ylabel='Dollar Value (Thousands)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We're looking for clean separation between clusters\n",
    "    - All of them have relatively clean separation between eachother so this is a difficult choice\n",
    "    - We'll go with 3 clusters to keep things simple while also having a larger variety of clusters to work with than the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we'll create our 3 cluster types and add them to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making copy of train so we don't alter the original\n",
    "train_k1 = train.copy()\n",
    "\n",
    "# creating kmeans object and fitting to data\n",
    "kmeans_1 = KMeans(n_clusters=3, random_state = 123)\n",
    "kmeans_1.fit(X_c1)\n",
    "\n",
    "# creating clusters and adding as column\n",
    "train_k1['cluster'] = kmeans_1.predict(X_c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we're going to examine our clusters in-depth and identify what they each represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setting figure and text size \n",
    "sns.set(rc = {'figure.figsize':(8,8)})\n",
    "sns.set(font_scale = 2)\n",
    "\n",
    "# plotting data, setting title and scaling y axis values to simplify them\n",
    "sns.set(font_scale = 2)\n",
    "sns.scatterplot(x = train_k1['property_sq_ft'], y = train_k1['tax_dollar_value'] / 1000, data = train_k1, hue = 'cluster', palette = 'tab10').set_title(\"Property SqFt. and Tax Dollar Value in 3 Clusters\")\n",
    "plt.xlabel(\"Property SqFt.\")\n",
    "plt.ylabel(\"Tax Value in Dollars (Thousands)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cluster 0 is mostly \n",
    "    - low value\n",
    "    - small-size properties\n",
    "\n",
    "\n",
    "- Cluster 1 is mostly \n",
    "    - mid-to-high value \n",
    "    - medium-to-large size properties\n",
    "    \n",
    "    \n",
    "- Cluster 2 is mostly \n",
    "    - low-to-mid value \n",
    "    - small-to-medium size properties "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Feature Set 1 - Hypothesis Test\n",
    "\n",
    "Now that we've identified the characterstics of our properties, we'll perform an ANOVA test to see if there are any differences between their average log errors.\n",
    "\n",
    "- H0 = There is no difference between the average log error of cluster 0, cluster 1, and cluster 2\n",
    "- Ha = There is a difference between the average log error of cluster 0, cluster 1, and cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating an array for the log error of each cluster type\n",
    "cluster_0 = train_k1[train_k1.cluster == 0].log_error\n",
    "cluster_1 = train_k1[train_k1.cluster == 1].log_error\n",
    "cluster_2 = train_k1[train_k1.cluster == 2].log_error\n",
    "\n",
    "# performing ANOVA test\n",
    "f, p = stats.f_oneway(cluster_0, cluster_1, cluster_2)\n",
    "\n",
    "# printing results\n",
    "print(f'alpha = .05\\n')\n",
    "print(f'f = {f:.5f}')\n",
    "print (f'p = {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- p is much greater than alpha\n",
    "- We fail to reject our null hypothesis and conclude there is no difference in log error between the clusters\n",
    "- In light of this, we will not be using this cluster for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Feature Set 2 - property_sq_ft | bedroom_count <a name=\"ce\"></a>\n",
    "\n",
    "Let's cluster using these two features and check if there are any differences between their log error values.\n",
    "\n",
    "To begin, well use subplots to see what our clusters look like with different cluster amounts. This gives us a good idea of how many clusters we should choose for a given pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating DFs with the two features we want to cluster\n",
    "# scaled for clustering, unscaled for plotting\n",
    "X_c2 = train_scaled[['bedroom_count','property_sq_ft']]\n",
    "X_c2_unscaled = train[['bedroom_count','property_sq_ft']]\n",
    "\n",
    "# setting up subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12,12), sharex=True, sharey=True)\n",
    "\n",
    "# creating clusters, number of clusters increasing from 2 - 5\n",
    "for ax, k in zip(axs.ravel(), range(2, 6)):\n",
    "    clusters = KMeans(k).fit(X_c2).predict(X_c2)\n",
    "    X_c2_unscaled['clusters'] = clusters\n",
    "    plt.suptitle('Varying K Values For Cluster: property_sq_ft | bedroom_count',fontsize=25)\n",
    "    \n",
    "    # plotting \n",
    "    sns.set(font_scale = 2)\n",
    "    ax.scatter(X_c2_unscaled.bedroom_count, X_c2_unscaled.property_sq_ft, c=clusters, cmap = 'tab10')\n",
    "    ax.set(title='Cluster Amount = {}'.format(k), ylabel='Property Size (SqFt.)', xlabel = '# of Bedrooms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once again, we're looking for good separation between clusters\n",
    "    - In this instance, cluster amounts 2 and 3 have the best separation\n",
    "        - We'll stick with our strategy of using 3 clusters to make things simple while also having more than the minimum number of clusters\n",
    "- Property square feet seems to be having a strong influence on our clusters since it was in both feature sets and we're seeing similar splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we'll create our 3 cluster types and add them to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making copy of train so we don't alter the original\n",
    "train_k2 = train.copy()\n",
    "\n",
    "# creating kmeans object and fitting to data\n",
    "kmeans_2 = KMeans(n_clusters=3, random_state=123)\n",
    "kmeans_2.fit(X_c2)\n",
    "\n",
    "# creating clusters and adding as column\n",
    "train_k2['cluster'] = kmeans_2.predict(X_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we're going to examine our clusters in-depth and identify what they each represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting figure and text size \n",
    "sns.set(rc = {'figure.figsize':(8,8)})\n",
    "sns.set(font_scale = 2)\n",
    "\n",
    "# plotting data, setting title and scaling y axis values to simplify them\n",
    "sns.scatterplot(x = train_k2['bedroom_count'], y = train_k2['property_sq_ft'], data = train_k2, hue = 'cluster', palette = 'tab10').set_title(\"Property SqFt. and Bedroom Count in 3 Clusters\")\n",
    "plt.ylabel(\"Property Size (SqFt.)\")\n",
    "plt.xlabel(\"# of Bedrooms in Property\")\n",
    "plt.xticks(np.arange(0, 12, 1.0))\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cluster 0 is mostly \n",
    "    - low bedroom count\n",
    "    - small size properties\n",
    "    \n",
    "    \n",
    "- Cluster 1 is mostly\n",
    "    - medium-to-high bedroom count\n",
    "    - medium-to-large size properties\n",
    "    \n",
    "    \n",
    "- Cluster 2 is mostly\n",
    "    - low-to-medium bedroom count \n",
    "    - small size properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Feature Set 2 - Hypothesis Test - ANOVA\n",
    "\n",
    "Now that we've identified the characterstics of our properties, we'll perform an ANOVA test to see if there is any difference in their average log errors.\n",
    "\n",
    "- H0 = There is no difference between the average log error of cluster 0, cluster 1, and cluster 2\n",
    "- Ha = There is a difference between the average log error of cluster 0, cluster 1, and cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an array for the log error of each cluster type\n",
    "cluster_0 = train_k2[train_k2.cluster == 0].log_error\n",
    "cluster_1 = train_k2[train_k2.cluster == 1].log_error\n",
    "cluster_2 = train_k2[train_k2.cluster == 2].log_error\n",
    "\n",
    "# performing ANOVA test\n",
    "f, p = stats.f_oneway(cluster_0, cluster_1, cluster_2)\n",
    "\n",
    "# printing results\n",
    "print(f'alpha = .05\\n')\n",
    "print (f'p = {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- p is less than alpha\n",
    "- We reject our null hypothesis and conclude there is a difference in the average log error of the clusters\n",
    "- Since these clusters appear to relate to log error, we'll use them as features in our modeling phase\n",
    "\n",
    "[Jump to Modeling (Presentation Use Only)](#bm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLUSTER FEATURE SET 3 - Bedroom_Count | Tax_Dollar_Value \n",
    "\n",
    "Let's cluster using these two features and check if there is any difference between their log error values.\n",
    "\n",
    "To begin, well use subplots to see what are clusters look like with different cluster amounts. This gives us a good idea of how many clusters we should choose for a given pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating df with the two features we want to cluster\n",
    "# scaled for clustering, unscaled for plotting\n",
    "X_c3 = train_scaled[['bedroom_count','tax_dollar_value']]\n",
    "X_c3_unscaled = train[['bedroom_count','tax_dollar_value']]\n",
    "\n",
    "# setting up subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12,12), sharex=True, sharey=True)\n",
    "\n",
    "# creating clusters, number of clusters increasing from 2 - 5\n",
    "for ax, k in zip(axs.ravel(), range(2, 6)):\n",
    "    clusters = KMeans(k).fit(X_c3).predict(X_c3)\n",
    "    X_c3_unscaled['clusters'] = clusters\n",
    "    plt.suptitle('Varying K Values for Cluster: tax_dollar_value | bedroom_count',fontsize=25)\n",
    "    \n",
    "    # plotting \n",
    "    sns.set(font_scale = 2)\n",
    "    ax.scatter(X_c3_unscaled.bedroom_count, X_c3_unscaled.tax_dollar_value / 1000, c = clusters, cmap = 'tab10')\n",
    "    ax.set(title='Cluster Amount = {}'.format(k), ylabel='Dollar Value (Thousands)', xlabel='# of Bedrooms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Still looking for clean separation and only cluster counts 2 and 3 depict this\n",
    "    - Once again we'll opt for 3 to keep things simple while also have an extra cluster beyond the minimum to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we'll create our 3 cluster types and add them to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making copy of train so we don't alter the original\n",
    "train_k3 = train.copy()\n",
    "\n",
    "# creating kmeans object and fitting to data\n",
    "kmeans_3 = KMeans(n_clusters=3, random_state=123)\n",
    "kmeans_3.fit(X_c3)\n",
    "\n",
    "# creating clusters and adding as column\n",
    "train_k3['cluster'] = kmeans_3.predict(X_c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we're going to examine our clusters in-depth and identify what they each represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting figure and text size \n",
    "sns.set(rc = {'figure.figsize':(8,8)})\n",
    "sns.set(font_scale = 2)\n",
    "\n",
    "# plotting data, setting title and scaling y axis values to simplify them\n",
    "sns.scatterplot(x = train_k3['bedroom_count'], y = train_k3['tax_dollar_value']/1000, data = train_k3, hue = 'cluster', palette = 'tab10').set_title(\"Tax Value and Bedroom Count in 3 Clusters\")\n",
    "plt.ylabel(\"Dollar Value (Thousands)\")\n",
    "plt.xlabel(\"# of Bedrooms in Property\")\n",
    "plt.xticks(np.arange(0, 12, 1.0))\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cluster 0 is mostly\n",
    "    - Low bedroom count, low value properties\n",
    "\n",
    "\n",
    "- Cluster 1 is mostly\n",
    "    - Low-to-medium bedroom count, low-to-medium value properties\n",
    "    \n",
    "    \n",
    "- Cluster 2 is mostly\n",
    "    - Low-to-medium bedroom count, medium-to-high value properties\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Feature set 3 - Hypothesis Test\n",
    "\n",
    "Now that we've identified the characterstics of our properties, we'll perform an ANOVA test to see if there is any difference between their average log errors.\n",
    "\n",
    "- H0 = There is no difference between the average log error of cluster 0, cluster 1, and cluster 2\n",
    "- Ha = There is a difference between the average log error of cluster 0, cluster 1, and cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an array for the log error of each cluster type\n",
    "cluster_0 = train_k3[train_k3.cluster == 0].log_error\n",
    "cluster_1 = train_k3[train_k3.cluster == 1].log_error\n",
    "cluster_2 = train_k3[train_k3.cluster == 2].log_error\n",
    "\n",
    "# performing ANOVA test\n",
    "f, p = stats.f_oneway(cluster_0, cluster_1, cluster_2)\n",
    "\n",
    "# printing results\n",
    "print(f'alpha = .05\\n')\n",
    "print(f'f = {f:.5f}')\n",
    "print (f'p = {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- p is much greater than alpha\n",
    "\n",
    "- We fail to reject our null hypothesis and conclude there is no difference in log error between the clusters\n",
    "\n",
    "- For these reasons we will not be using this cluster for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Non-Clustered Exploration__\n",
    "\n",
    "\n",
    "- Visualization and hypothesis tests suggest that the following features are drivers of log_error\n",
    "    - property_sq_ft\n",
    "    - tax_dollar_value\n",
    "    - bedroom_count\n",
    "\n",
    "\n",
    "__Clustered Exploration__\n",
    "\n",
    "\n",
    "- Created 3 clusters for each pair of features below\n",
    "        - Feature set 1: property_sq_ft | tax_dollar_value\n",
    "        - Feature set 2: property_sq_ft | bedroom_count\n",
    "        - Feature set 3: bedroom_count | tax_dollar_value\n",
    "\n",
    "\n",
    "- Ran ANOVA tests on each pair's clusters to determine if there were any differences in log error\n",
    "    - Only the clusters created using property_sq_ft and bedroom_count showed differences in log error\n",
    "        - If log errors meaningfully fluctuate between these cluster types, they may be useful as features in our model to predict log error \n",
    "\n",
    "\n",
    "__Moving Forward__\n",
    "\n",
    "\n",
    "- We will now move into modeling with the following as prospective features\n",
    "    - tax_dollar_value\n",
    "    - property_sq_ft\n",
    "    - bedroom_count\n",
    "    - Feature set 2's clusters\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling<a name=\"modeling\"></a>\n",
    "We'll now construct several models that will attempt to predict the log error associated with each property. \n",
    "\n",
    "We'll also create a __model.py__ file that contains the custom functions used in this phase.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN\n",
    "We'll create a baseline model that our other models must outperform.\n",
    "\n",
    "Next we'll create 3 alternate models. The 2 best that outperform our baseline will be taken to validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model<a name=\"bm\"></a>\n",
    "Our baseline model that will always predict log error to be the sample's average log error. \n",
    "\n",
    "The RMSE value is predictions produce will also serve as the benchmark that our alternate models will seek to outperform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_function(train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Baseline RMSE is .154544 and the metric our other models need to improve upon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 (All Feature Model)\n",
    "Overview: Let's create a model that uses all of the features from our exploration phase including clusters.\n",
    "\n",
    "Model type: Linear Regression\n",
    "\n",
    "\n",
    "Model Features \n",
    "- property_sq_ft\n",
    "- tax_dollar_value\n",
    "- bedroom_count\n",
    "- clusters\n",
    "    - cluster features: property_sq_ft and bedroom_count\n",
    "    - number of clusters: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_1_function(train_scaled, train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RMSE value is below baseline's (an improvement)\n",
    "- If this model outperforms either of the other models it will go to validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 (No Clusters Model)\n",
    "Overview: Let's create a model that uses all of the features from our exploration phase without any clusters.\n",
    "\n",
    "Model type: Linear Regression\n",
    "\n",
    "\n",
    "Model Features \n",
    "- property_sq_ft\n",
    "- tax_dollar_value\n",
    "- bedroom_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_function(train_scaled, train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model outperformed baseline but worse than model 1 which used clusters\n",
    "    - This suggests clusters are effective at improving log error predictions\n",
    "    \n",
    "[Jump to Conclusion (For Presentation Use Only)](#con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 (All Clusters Model)\n",
    "Overview: Since our last model performed worse without clusters, let's see how a model will perform with only clusters. It's possible that the clusters are so effective that the non-cluster features are creating noise which is worsening performance.\n",
    "\n",
    "Model type: Linear Regression\n",
    "\n",
    "\n",
    "Model Features \n",
    "- clusters\n",
    "    - features: property_sq_ft and bedroom_count\n",
    "    - number of clusters: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_function(train_scaled, train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This model outperformed the baseline but worse than our other 2 models\n",
    "- We're only taking our best 2 models to validation so this one will be left behind\n",
    "- It appears that our model performs better non-cluster features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALIDATE\n",
    "Our top 2 models from the train dataset phase will now be used on the validate dataset.\n",
    "\n",
    "The best model model will be taken to the test phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 (All Feature Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_1_function(train_scaled, validate_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 (No Clusters Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_function(train_scaled, validate_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This gives us an idea of how our models will perform on unseen data (out of sample)\n",
    "- Model 2 outperformed model 1 so it will be taken to the test phase\n",
    "- Interesting to note that our cluster model was outperformed better by our non-cluster model in validate (out of sample) but no on train (in sample)\n",
    "    - One possible explanation is that by chance, the feature data used to make the clusters (bedroom_count and property_sq_ft) in our validate sample is much different than our train sample, so model 1's performance was more heavily affected by its unfamiliarity with this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST\n",
    "Our best model from the validate phase will now be used on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 (No Clusters Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_function(train_scaled, test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our model performed worse on the test data than on the validate data.\n",
    "    - This is very interesting considering that both of validate and test are out-of-sample datasets\n",
    "    - A possible explanation for this could be that by chance, the data from train and validate was very similar and so the model performed better on validate since its data was similar to the data it was fitted on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created 4 models \n",
    "    \n",
    "- Baseline\n",
    "    - Always predicts average logerror\n",
    "        \n",
    "        \n",
    "- Model 1 (All features)\n",
    "    - Linear Regression\n",
    "    - Features \n",
    "        - property_sq_ft\n",
    "        - tax_dollar_value\n",
    "        - bedroom_count\n",
    "        - clusters\n",
    "            - cluster features: property_sq_ft and bedroom_count\n",
    "            - number of clusters: 3\n",
    "                \n",
    "                \n",
    "- Model 2 (No clusters)\n",
    "    - Linear Regression\n",
    "    - Features\n",
    "        - property_sq_ft\n",
    "        - tax_dollar_value\n",
    "        - bedroom_count\n",
    "            \n",
    "            \n",
    "- Model 3 (All clusters)\n",
    "    - Linear Regression\n",
    "    - Features \n",
    "        - clusters\n",
    "            - cluster features: property_sq_ft and bedroom_count\n",
    "            - number of clusters: 3\n",
    "        \n",
    "Model 2 performance details\n",
    "- Proved to be best model by outperforming the other models with regard to RMSE value on predictions for log error\n",
    "- Performed better on validate sample than on test sample\n",
    "    - Possible explanation is that by chance, the sample it was fitted on, train, had very similar data to the validate sample, but not the test sample.\n",
    "    \n",
    "All custom functions used in this phase are available in the __model.py__ file located in this repository.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Conclusion and Takeaways<a name=\"con\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Summary of Findings__\n",
    "\n",
    "Through visualizations, hypothesis tests and modeling, we discovered evidence that drivers of log_error include \n",
    "- bedroom_count\n",
    "- property_sq_ft\n",
    "- tax_dollar_value\n",
    "\n",
    "Clusters created from a combination of bedroom_count and property_sq_ft are viable feature for predicting log error\n",
    "\n",
    "We created several models including a baseline that always predicted logerror to be the sample average\n",
    "\n",
    "- Each model's performance was evaluated based on the RMSE value produced by comparing its prediction of logerror values vs. actual log error values\n",
    "\n",
    "\n",
    "- Model 2 was the best performer (specs listed below)\n",
    "\n",
    "    - Type: Linear Regression\n",
    "    \n",
    "    - Features: Uses all features listed above, except for clusters\n",
    "    \n",
    "    \n",
    "- Although this model did not use clusters as features and outperformed models that did, our clustering algorithm is very unrefined. With time it could be improved and incorporated into this model to improve its performance\n",
    "\n",
    "\n",
    "- It should be noted that our second best model used clusters and was only outperformed by our non-cluster model by a small margine. This is further evidence that clusters may still be useful for predicting log errors.\n",
    "    \n",
    "__Recommendation__\n",
    "- Begin a project to improve the accuracy of our zillow estimate software using the insights and model generated from this project\n",
    "\n",
    "\n",
    "__Expectations__\n",
    "- By improving the accuracy of our zestimates we will increase satisfaction among our current users and make our services more attractive to potential users. \n",
    "\n",
    "\n",
    "__In the future__\n",
    "- I'd like to revisit this project and explore / model with clusters more. A new combination of cluster features may generate clusters that prove to be very useful in predicting log error. I'd also like to try imputing some of the null values we dropped and observe how that influences our hypothesis tests and modeling.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
